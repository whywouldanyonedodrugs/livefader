"""
live_trader.py ‚Äì v3.0 (Simplified & Corrected)
===========================================================================
This version restores the missing _load_listing_dates method that was
accidentally deleted, fixing the startup crash.
"""

from __future__ import annotations

import secrets
import asyncio
import dataclasses
import json
import logging
import os
import signal as sigmod
import sys
import traceback
from datetime import datetime, timedelta, timezone
from pathlib import Path
from collections import defaultdict
from typing import Any, Dict, List, Optional, Tuple
import subprocess

import aiohttp
import asyncpg
import ccxt.async_support as ccxt
import joblib
import pandas as pd
import numpy as np
import statsmodels.api as sm
import yaml
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

import config as cfg
from . import indicators as ta
from . import filters
from . shared_utils import is_blacklisted
from watchdog.events import FileSystemEventHandler
from watchdog.observers import Observer
from .exchange_proxy import ExchangeProxy
from .database import DB
from .telegram import TelegramBot


from pydantic import Field, Extra, ValidationError
from pydantic_settings import BaseSettings

def triangular_moving_average(series: pd.Series, period: int) -> pd.Series:
    """Calculates a Triangular Moving Average (TMA). Helper function."""
    # This is a simplified and faster version of TMA
    return series.rolling(window=period, min_periods=period).mean().rolling(window=period, min_periods=period).mean()

class RegimeDetector:
    """
    Calculates and caches the market regime based on a live benchmark asset.
    """
    def __init__(self, exchange, config: dict):
        self.exchange = exchange
        self.cfg = config
        self.benchmark_symbol = self.cfg.get("REGIME_BENCHMARK_SYMBOL", "BTCUSDT")
        self.cache_duration = timedelta(minutes=self.cfg.get("REGIME_CACHE_MINUTES", 60))
        self.cached_regime = "UNKNOWN"
        self.last_calculation_time = None
        LOG.info("RegimeDetector initialized for benchmark %s with a %d-minute cache.", self.benchmark_symbol, self.cfg.get("REGIME_CACHE_MINUTES", 60))

    async def _fetch_benchmark_data(self) -> pd.DataFrame | None:
        """Fetches the last ~500 days of daily OHLCV data for the benchmark symbol."""
        try:
            # Fetch enough data for the long-term TMA and Markov model
            ohlcv = await self.exchange.fetch_ohlcv(self.benchmark_symbol, '1d', limit=500)
            if len(ohlcv) < 200: # Need at least ~200 days for meaningful calculation
                LOG.warning("Not enough historical data for benchmark %s to calculate regime. Found %d bars.", self.benchmark_symbol, len(ohlcv))
                return None
            
            df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)
            df.set_index('timestamp', inplace=True)
            return df
        except Exception as e:
            LOG.error("Failed to fetch benchmark data for regime detection: %s", e)
            return None

    def _calculate_vol_regime(self, daily_returns: pd.Series) -> pd.Series:
        """Calculates the volatility regime using a Markov Switching Model."""
        try:
            model = sm.tsa.MarkovRegression(daily_returns.dropna(), k_regimes=2, switching_variance=True)
            results = model.fit(disp=False)
            low_vol_regime_idx = np.argmin(results.params[-2:])
            vol_regimes = np.where(results.smoothed_marginal_probabilities[low_vol_regime_idx] > 0.5, "LOW_VOL", "HIGH_VOL")
            # Return as a pandas Series with the same index as the input
            return pd.Series(vol_regimes, index=daily_returns.dropna().index, name="vol_regime")
        except Exception as e:
            LOG.warning("Could not fit Markov model for volatility regime: %s. Defaulting to UNKNOWN.", e)
            return pd.Series("UNKNOWN", index=daily_returns.index, name="vol_regime")

    def _calculate_trend_regime(self, df_daily: pd.DataFrame) -> pd.Series:
        """Calculates the trend regime using a TMA and Keltner Channel."""
        df_daily['tma'] = triangular_moving_average(df_daily['close'], self.cfg.get("REGIME_MA_PERIOD", 100))
        atr_series = ta.atr(df_daily, period=self.cfg.get("REGIME_ATR_PERIOD", 20))
        df_daily['keltner_upper'] = df_daily['tma'] + (atr_series * self.cfg.get("REGIME_ATR_MULT", 2.0))
        df_daily['keltner_lower'] = df_daily['tma'] - (atr_series * self.cfg.get("REGIME_ATR_MULT", 2.0))
        df_daily.dropna(inplace=True)

        trend = pd.Series(np.nan, index=df_daily.index, dtype="object")
        for i in range(1, len(df_daily)):
            if df_daily['close'].iloc[i] > df_daily['keltner_upper'].iloc[i]:
                trend.iloc[i] = "BULL"
            elif df_daily['close'].iloc[i] < df_daily['keltner_lower'].iloc[i]:
                trend.iloc[i] = "BEAR"
            else:
                trend.iloc[i] = trend.iloc[i-1] # Carry forward the previous trend
        
        return trend.ffill().bfill()

    async def get_current_regime(self) -> str:
        """The main public method to get the current market regime, using a cache."""
        now = datetime.now(timezone.utc)
        if self.last_calculation_time and (now - self.last_calculation_time) < self.cache_duration:
            return self.cached_regime

        LOG.info("Regime cache expired or empty. Calculating new market regime...")
        self.last_calculation_time = now # Update time immediately to prevent re-entry

        df_daily = await self._fetch_benchmark_data()
        if df_daily is None:
            self.cached_regime = "UNKNOWN"
            return self.cached_regime

        daily_returns = df_daily['close'].pct_change()
        
        vol_regime = self._calculate_vol_regime(daily_returns)
        df_daily['vol_regime'] = vol_regime
        
        trend_regime = self._calculate_trend_regime(df_daily)
        df_daily['trend_regime'] = trend_regime
        
        df_daily.dropna(subset=['vol_regime', 'trend_regime'], inplace=True)
        
        if df_daily.empty:
            LOG.warning("Regime calculation resulted in an empty DataFrame. Cannot determine regime.")
            self.cached_regime = "UNKNOWN"
            return self.cached_regime

        latest_regime = f"{df_daily['trend_regime'].iloc[-1]}_{df_daily['vol_regime'].iloc[-1]}"
        self.cached_regime = latest_regime
        
        LOG.info("New market regime calculated: %s", self.cached_regime)
        return self.cached_regime

def create_unique_cid(tag: str) -> str:
    """Creates a globally unique client order ID for one-shot orders like entry."""
    timestamp_ms = int(datetime.now(timezone.utc).timestamp() * 1000)
    # The random hex makes it unique even if called in the same millisecond.
    random_suffix = secrets.token_hex(2)
    return f"bot_{tag}_{timestamp_ms}_{random_suffix}"[:36]

def create_stable_cid(pid: int, tag: str) -> str:
    """Creates a stable, repeatable client order ID for manageable orders like SL/TP."""
    return f"bot_{pid}_{tag}"[:36]

@dataclasses.dataclass
class Signal:
    # --- All required fields (no default value) ---
    symbol: str
    entry: float
    atr: float
    rsi: float
    adx: float
    atr_pct: float
    market_regime: str
    price_boom_pct: float
    price_slowdown_pct: float
    vwap_dev_pct: float
    ret_30d: float
    ema_fast: float
    ema_slow: float
    listing_age_days: int
    vwap_z_score: float
    is_ema_crossed_down: bool
    vwap_consolidated: bool
    session_tag: str
    day_of_week: int
    hour_of_day: int

    # --- All optional fields (with a default value) ---
    win_probability: float = 0.0

    def __post_init__(self):
        """This function runs after the object is created."""
        LOG.info(f"Successfully created Signal object for {self.symbol} using correct class definition.")




LISTING_PATH = Path("listing_dates.json")

###############################################################################
# 0 ‚ñ∏ LOGGING #################################################################
###############################################################################

LOG = logging.getLogger("live_trader")
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

logging.getLogger("ccxt").setLevel(logging.WARNING)
logging.getLogger("aiohttp").setLevel(logging.WARNING)

###############################################################################
# 1 ‚ñ∏ SETTINGS pulled from ENV (.env) #########################################
###############################################################################

class Settings(BaseSettings):
    """Secrets & env flags."""
    bybit_api_key: str = Field(..., validation_alias="BYBIT_API_KEY")
    bybit_api_secret: str = Field(..., validation_alias="BYBIT_API_SECRET")
    bybit_testnet: bool = Field(False, validation_alias="BYBIT_TESTNET")
    tg_bot_token: str = Field(..., validation_alias="TG_BOT_TOKEN")
    tg_chat_id: str = Field(..., validation_alias="TG_CHAT_ID")
    pg_dsn: str = Field(..., validation_alias="DATABASE_URL")
    default_leverage: int = 10

    class Config:
        env_file = ".env"
        case_sensitive = False
        extra = 'ignore'

###############################################################################
# 2 ‚ñ∏ PATHS & YAML LOADER #####################################################
###############################################################################

CONFIG_PATH = Path("config.yaml")
SYMBOLS_PATH = Path("symbols.txt")

def load_yaml(p: Path) -> Dict[str, Any]:
    if not p.exists():
        raise FileNotFoundError(p)
    return yaml.safe_load(p.read_text()) or {}

###############################################################################
# 3 ‚ñ∏ HOT‚ÄëRELOAD WATCHER ######################################################
###############################################################################

class _Watcher(FileSystemEventHandler):
    def __init__(self, path: Path, cb):
        self.path = path.resolve()
        self.cb = cb
        obs = Observer()
        obs.schedule(self, self.path.parent.as_posix(), recursive=False)
        obs.daemon = True
        obs.start()

    def on_modified(self, e):
        if Path(e.src_path).resolve() == self.path:
            self.cb()

###############################################################################
# 4 ‚ñ∏ RISK MANAGER ###########################################################
###############################################################################

class RiskManager:
    def __init__(self, cfg_dict: Dict[str, Any]):
        self.cfg = cfg_dict
        self.loss_streak = 0
        self.kill_switch = False

    async def on_trade_close(self, pnl: float, telegram: TelegramBot):
        if pnl < 0:
            self.loss_streak += 1
        else:
            self.loss_streak = 0
        if self.loss_streak >= self.cfg["MAX_LOSS_STREAK"]:
            self.kill_switch = True
            await telegram.send("‚ùå Kill‚Äëswitch: max loss streak")

    def can_trade(self) -> bool:
        return not self.kill_switch

###############################################################################
# 5 ‚ñ∏ MAIN TRADER ###########################################################
###############################################################################

class LiveTrader:
    def __init__(self, settings: Settings, cfg_dict: Dict[str, Any]):
        self.settings = settings
        self.cfg = {}
        for key in dir(cfg):
            if key.isupper():
                self.cfg[key] = getattr(cfg, key)
        self.cfg.update(cfg_dict)

        for k, v in self.cfg.items():
            setattr(cfg, k, v)

        self.db = DB(settings.pg_dsn)
        self.tg = TelegramBot(settings.tg_bot_token, settings.tg_chat_id)
        self.risk = RiskManager(self.cfg)
        
        # FIX: Create self.exchange FIRST...
        self.exchange = ExchangeProxy(self._init_ccxt())
        
        # FIX: ...THEN create self.regime_detector, which depends on it.
        self.regime_detector = RegimeDetector(self.exchange, self.cfg)

        self.symbols = self._load_symbols()
        self.open_positions: Dict[int, Dict[str, Any]] = {}
        self.peak_equity: float = 0.0
        self._listing_dates_cache: Dict[str, datetime.date] = {}
        self.last_exit: Dict[str, datetime] = {}
        _Watcher(CONFIG_PATH, self._reload_cfg)
        _Watcher(SYMBOLS_PATH, self._reload_symbols)
        self.paused = False
        self.tasks: List[asyncio.Task] = []
        self.api_semaphore = asyncio.Semaphore(10)
        self.symbol_locks: Dict[str, asyncio.Lock] = defaultdict(asyncio.Lock)

        self.win_prob_model = None
        model_path = Path("win_probability_model.joblib")
        if model_path.exists():
            try:
                self.win_prob_model = joblib.load(model_path)
                LOG.info(f"Successfully loaded predictive model from {model_path}")
            except Exception as e:
                LOG.error(f"Failed to load predictive model: {e}")
        else:
            LOG.warning(f"Predictive model file not found at {model_path}. Win probability scoring will be disabled.")

    def _init_ccxt(self):
        ex = ccxt.bybit({
            "apiKey": self.settings.bybit_api_key,
            "secret": self.settings.bybit_api_secret,
            "enableRateLimit": True,
            "options": {"defaultType": "swap"},
        })
        if self.settings.bybit_testnet:
            ex.set_sandbox_mode(True)
        return ex

    @staticmethod
    def _cid(pid: int, tag: str) -> str:
        """Creates a unique, valid client order ID."""
        timestamp_ms = int(datetime.now(timezone.utc).timestamp() * 1000)
        # Bybit V5 requires clientOrderId to be 36 chars or less.
        return f"bot_{pid}_{tag}_{timestamp_ms}"[:36]

    @staticmethod
    def _load_symbols():
        return SYMBOLS_PATH.read_text().split()

    def _reload_cfg(self):
        self.cfg.update(load_yaml(CONFIG_PATH))
        LOG.info("Config reloaded")

    def _reload_symbols(self):
        self.symbols = self._load_symbols()
        LOG.info("Symbols reloaded ‚Äì %d symbols", len(self.symbols))

    async def _ensure_leverage(self, symbol: str):
        try:
            await self.exchange.set_margin_mode("cross", symbol)
            await self.exchange.set_leverage(self.settings.default_leverage, symbol)
        except ccxt.ExchangeError as e:
            if "leverage not modified" in str(e):
                LOG.debug("Leverage for %s already set to %dx.", symbol, self.settings.default_leverage)
            else:
                LOG.warning("Leverage setup failed for %s: %s", symbol, e)
                raise e
        except Exception as e:
            LOG.warning("An unexpected error occurred during leverage setup for %s: %s", symbol, e)
            raise e

    # --- THIS METHOD WAS MISSING ---
    async def _load_listing_dates(self) -> Dict[str, datetime.date]:
        if LISTING_PATH.exists():
            import datetime as _dt
            raw = json.loads(LISTING_PATH.read_text())
            return {s: _dt.date.fromisoformat(ts) for s, ts in raw.items()}

        LOG.info("listing_dates.json not found. Fetching from exchange...")
        async def fetch_date(sym):
            try:
                candles = await self.exchange.fetch_ohlcv(sym, timeframe="1d", limit=1, since=0)
                if candles:
                    ts = datetime.fromtimestamp(candles[0][0] / 1000, tz=timezone.utc)
                    return sym, ts.date()
            except Exception as e:
                LOG.warning("Could not fetch listing date for %s: %s", sym, e)
            return sym, None

        tasks = [fetch_date(sym) for sym in self.symbols]
        results = await asyncio.gather(*tasks)
        out = {sym: dt for sym, dt in results if dt}
        LISTING_PATH.write_text(json.dumps({k: v.isoformat() for k, v in out.items()}, indent=2))
        LOG.info("Saved %d listing dates to %s", len(out), LISTING_PATH)
        return out

    async def _fetch_platform_balance(self) -> dict:
        account_type = self.cfg.get("BYBIT_ACCOUNT_TYPE", "STANDARD").upper()
        params = {}
        if account_type == "UNIFIED":
            params['accountType'] = 'UNIFIED'
        try:
            return await self.exchange.fetch_balance(params=params)
        except Exception as e:
            LOG.error("Failed to fetch %s account balance: %s", account_type, e)
            return {"total": {"USDT": 0.0}, "free": {"USDT": 0.0}}

    async def _risk_amount(self, free_usdt: float) -> float:
        mode = self.cfg.get("RISK_MODE", "PERCENT").upper()
        if mode == "PERCENT":
            return free_usdt * self.cfg["RISK_PCT"]
        return float(self.cfg["FIXED_RISK_USDT"])

    # --- NEW, CORRECTED HELPER METHODS ---
    async def _fetch_by_cid(self, cid: str, symbol: str):
        """Fetches an order using the robust generic fetch_order method."""
        return await self.exchange.fetch_order(
            None, symbol, params={"clientOrderId": cid, "category": "linear"}
        )

    async def _cancel_by_cid(self, cid: str, symbol: str):
        """Cancels an order using the robust generic cancel_order method."""
        try:
            return await self.exchange.cancel_order(
                None, symbol, params={"clientOrderId": cid, "category": "linear"}
            )
        except ccxt.OrderNotFound:
            LOG.warning("Order %s for %s already filled or cancelled, no action needed.", cid, symbol)
        except Exception as e:
            LOG.error("Failed to cancel order %s for %s: %s", cid, symbol, e)

    async def _all_open_orders(self, symbol: str) -> list:
        """
        Fetches all types of open orders (regular, conditional, TP/SL) for a specific symbol
        by correctly using the Bybit V5 API parameters.
        """
        params_linear = {'category': 'linear'}
        try:
            # Fetch orders for the specific symbol passed as an argument
            active_orders = await self.exchange.fetch_open_orders(symbol, params=params_linear)
            stop_orders = await self.exchange.fetch_open_orders(symbol, params={**params_linear, 'orderFilter': 'StopOrder'})
            tpsl_orders = await self.exchange.fetch_open_orders(symbol, params={**params_linear, 'orderFilter': 'tpslOrder'})
            return active_orders + stop_orders + tpsl_orders
        except Exception as e:
            LOG.warning("Could not fetch all open orders for %s: %s", symbol, e)
            return [] # Return an empty list on failure to prevent crashes

    async def _all_open_orders_for_all_symbols(self) -> list:
        """
        Fetches all types of open orders (regular, conditional, TP/SL) for all symbols.
        """
        params_linear = {'category': 'linear'}
        try:
            active_orders = await self.exchange.fetch_open_orders(params=params_linear)
            stop_orders = await self.exchange.fetch_open_orders(params={**params_linear, 'orderFilter': 'StopOrder'})
            tpsl_orders = await self.exchange.fetch_open_orders(params={**params_linear, 'orderFilter': 'tpslOrder'})
            return active_orders + stop_orders + tpsl_orders
        except Exception as e:
            LOG.warning("Could not fetch all open orders for all symbols: %s", e)
            return []

    async def _scan_symbol_for_signal(self, symbol: str, market_regime: str, eth_macd: Optional[dict]) -> Optional[Signal]:
        LOG.info("Checking %s...", symbol)
        try:
            base_tf = self.cfg.get('TIMEFRAME', '5m')
            ema_tf = self.cfg.get('EMA_TIMEFRAME', '4h')
            rsi_tf = self.cfg.get('RSI_TIMEFRAME', '1h')
            atr_tf = self.cfg.get('ADX_TIMEFRAME', '1h')
            required_tfs = {base_tf, ema_tf, rsi_tf, atr_tf, '1d'}

            async with self.api_semaphore:
                tasks = {tf: self.exchange.fetch_ohlcv(symbol, tf, limit=500) for tf in required_tfs}
                results = await asyncio.gather(*tasks.values(), return_exceptions=True)
                ohlcv_data = dict(zip(tasks.keys(), results))

            dfs = {}
            for tf, data in ohlcv_data.items():
                if isinstance(data, Exception) or not data:
                    LOG.debug("Could not fetch OHLCV for %s on %s timeframe.", symbol, tf)
                    return None
                df = pd.DataFrame(data, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
                df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms', utc=True)
                df.set_index('timestamp', inplace=True)
                
                recent_candles = df.tail(100)
                if not recent_candles.empty:
                    zero_volume_pct = (recent_candles['volume'] == 0).sum() / len(recent_candles)
                    if zero_volume_pct > 0.25:
                        LOG.warning("DATA_ERROR for %s on %s: Detected stale data (%.0f%% zero volume). Skipping symbol.", symbol, tf, zero_volume_pct * 100)
                        return None

                df.drop(df.index[-1], inplace=True)
                if df.empty: return None
                dfs[tf] = df

            df5 = dfs[base_tf]

            df5['ema_fast'] = ta.ema(dfs[ema_tf]['close'], cfg.EMA_FAST_PERIOD).reindex(df5.index, method='ffill')
            df5['ema_slow'] = ta.ema(dfs[ema_tf]['close'], cfg.EMA_SLOW_PERIOD).reindex(df5.index, method='ffill')
            df5['rsi'] = ta.rsi(dfs[rsi_tf]['close'], cfg.RSI_PERIOD).reindex(df5.index, method='ffill')
            df5['atr'] = ta.atr(dfs[atr_tf], cfg.ADX_PERIOD).reindex(df5.index, method='ffill')
            df5['adx'] = ta.adx(dfs[atr_tf], cfg.ADX_PERIOD).reindex(df5.index, method='ffill')

            tf_minutes = 5
            boom_bars = int((cfg.PRICE_BOOM_PERIOD_H * 60) / tf_minutes)
            slowdown_bars = int((cfg.PRICE_SLOWDOWN_PERIOD_H * 60) / tf_minutes)
            df5['price_boom_ago'] = df5['close'].shift(boom_bars)
            df5['price_slowdown_ago'] = df5['close'].shift(slowdown_bars)

            df1d = dfs['1d']
            ret_30d = (df1d['close'].iloc[-1] / df1d['close'].iloc[-cfg.STRUCTURAL_TREND_DAYS] - 1) if len(df1d) > cfg.STRUCTURAL_TREND_DAYS else 0.0

            vwap_bars = int((cfg.GAP_VWAP_HOURS * 60) / tf_minutes)
            vwap_num = (df5['close'] * df5['volume']).shift(1).rolling(vwap_bars).sum()
            vwap_den = df5['volume'].shift(1).rolling(vwap_bars).sum()
            df5['vwap'] = vwap_num / vwap_den
            vwap_dev_raw = df5['close'] - df5['vwap']
            df5['vwap_dev_pct'] = vwap_dev_raw / df5['vwap']
            df5['price_std'] = df5['close'].rolling(vwap_bars).std()
            df5['vwap_z_score'] = vwap_dev_raw / df5['price_std']
            df5['vwap_ok'] = df5['vwap_dev_pct'].abs() <= cfg.GAP_MAX_DEV_PCT
            df5['vwap_consolidated'] = df5['vwap_ok'].rolling(cfg.GAP_MIN_BARS).min().fillna(0).astype(bool)

            df5.dropna(inplace=True)
            if df5.empty: return None

            last = df5.iloc[-1]
            
            is_ema_crossed_down = last['ema_fast'] < last['ema_slow']
            now_utc = datetime.now(timezone.utc)
            boom_ret_pct = (last['close'] / last['price_boom_ago'] - 1)
            slowdown_ret_pct = (last['close'] / last['price_slowdown_ago'] - 1)
            price_boom = boom_ret_pct > cfg.PRICE_BOOM_PCT
            price_slowdown = slowdown_ret_pct < cfg.PRICE_SLOWDOWN_PCT
            atr_pct = (last['atr'] / last['close']) * 100 if last['close'] > 0 else 0.0
            listing_age_days = (now_utc.date() - self._listing_dates_cache[symbol]).days if symbol in self._listing_dates_cache else -1
            hour_of_day = now_utc.hour
            day_of_week = now_utc.weekday()
            session_tag = "ASIA" if 0 <= hour_of_day < 8 else "EUROPE" if 8 <= hour_of_day < 16 else "US"

            ema_down = True
            ema_log_msg = " DISABLED"
            if self.cfg.get("EMA_FILTER_ENABLED", True):
                ema_down = is_ema_crossed_down
                ema_log_msg = f"{'‚úÖ' if ema_down else '‚ùå'} (Fast: {last['ema_fast']:.4f} < Slow: {last['ema_slow']:.4f})"

            trend_log_msg = " DISABLED"
            if self.cfg.get("STRUCTURAL_TREND_FILTER_ENABLED", True):
                trend_ok = ret_30d <= self.cfg.get("STRUCTURAL_TREND_RET_PCT", 0.01)
                trend_log_msg = f"{'‚úÖ' if trend_ok else '‚ùå'} (Return: {ret_30d:+.2%})"

            vwap_log_msg = " DISABLED"
            if self.cfg.get("GAP_FILTER_ENABLED", True):
                vwap_dev_pct = last.get('vwap_dev_pct', float('nan'))
                vwap_dev_str = f"{vwap_dev_pct:.2%}" if pd.notna(vwap_dev_pct) else "N/A"
                current_dev_ok = last.get('vwap_ok', False)
                vwap_log_msg = f"{'‚úÖ' if last['vwap_consolidated'] else '‚ùå'} (Streak Failed) | Current Dev: {'‚úÖ' if current_dev_ok else '‚ùå'} ({vwap_dev_str})"

            # --- THIS IS THE LOGGING BLOCK THAT WAS MISSING ---
            LOG.debug(
                f"\n--- {symbol} | {last.name.strftime('%Y-%m-%d %H:%M')} UTC ---\n"
                f"  [Base Timeframe: {base_tf}]\n"
                f"  - Price Boom     (>{cfg.PRICE_BOOM_PCT:.0%}, {cfg.PRICE_BOOM_PERIOD_H}h lookback): {'‚úÖ' if price_boom else '‚ùå'} (is {boom_ret_pct:+.2%})\n"
                f"  - Price Slowdown (<{cfg.PRICE_SLOWDOWN_PCT:.0%}, {cfg.PRICE_SLOWDOWN_PERIOD_H}h lookback): {'‚úÖ' if price_slowdown else '‚ùå'} (is {slowdown_ret_pct:+.2%})\n"
                f"  - EMA Trend Down ({ema_tf}):      {ema_log_msg}\n"
                f"  --------------------------------------------------\n"
                f"  - RSI ({rsi_tf}):                 {last['rsi']:.2f} (Veto: {not (cfg.RSI_ENTRY_MIN <= last['rsi'] <= cfg.RSI_ENTRY_MAX)})\n"
                f"  - 30d Trend Filter:        {trend_log_msg}\n"
                f"  - VWAP Consolidated:       {vwap_log_msg}\n"
                f"  - ATR ({atr_tf}):                 {last['atr']:.6f}\n"
                f"====================================================\n"
            )
            # --- END OF MISSING BLOCK ---

            if price_boom and price_slowdown and ema_down:
                LOG.info("SIGNAL FOUND for %s at price %.4f", symbol, last['close'])
                
                signal_obj = Signal(
                    symbol=symbol, entry=float(last['close']), atr=float(last['atr']),
                    rsi=float(last['rsi']), adx=float(last['adx']), atr_pct=atr_pct,
                    market_regime=market_regime, price_boom_pct=boom_ret_pct,
                    price_slowdown_pct=slowdown_ret_pct, vwap_dev_pct=float(last.get('vwap_dev_pct', 0.0)),
                    vwap_z_score=float(last.get('vwap_z_score', 0.0)), ret_30d=ret_30d,
                    ema_fast=float(last['ema_fast']), ema_slow=float(last['ema_slow']),
                    listing_age_days=listing_age_days, session_tag=session_tag,
                    day_of_week=day_of_week, hour_of_day=hour_of_day,
                    vwap_consolidated=bool(last.get('vwap_consolidated', False)),
                    is_ema_crossed_down=bool(is_ema_crossed_down)
                )

                if self.win_prob_model:
                    try:
                        # 1. Get the exact feature names the model was trained on (excluding the 'const')
                        model_features = self.win_prob_model.model.exog_names[1:]
                        
                        # 2. Create a dictionary from the live signal data
                        live_data = {
                            'rsi_at_entry': signal_obj.rsi,
                            'adx_at_entry': signal_obj.adx,
                            'price_boom_pct_at_entry': signal_obj.price_boom_pct,
                            'price_slowdown_pct_at_entry': signal_obj.price_slowdown_pct,
                            'vwap_z_at_entry': signal_obj.vwap_z_score,
                            'ema_spread_pct_at_entry': (signal_obj.ema_fast - signal_obj.ema_slow) / signal_obj.ema_slow if signal_obj.ema_slow > 0 else 0,
                            'is_ema_crossed_down_at_entry': int(signal_obj.is_ema_crossed_down), # Convert bool to int
                            'day_of_week_at_entry': signal_obj.day_of_week,
                            'hour_of_day_at_entry': signal_obj.hour_of_day
                        }
                        
                        # 3. Create a DataFrame from this dictionary, and explicitly order the columns
                        #    to perfectly match the training data.
                        features_df = pd.DataFrame([live_data], columns=model_features)
                        
                        # 4. Add the constant and predict
                        features_df = sm.add_constant(features_df, prepend=True, has_constant='add')
                        win_prob = self.win_prob_model.predict(features_df.values)[0]
                        signal_obj.win_probability = float(win_prob)
                        
                    except Exception as e:
                        LOG.warning(f"Failed to score signal for {symbol}: {e}")
                # --- END OF FIX ---
                
                return signal_obj

            return None

        except ccxt.BadSymbol:
            LOG.warning("Could not scan %s: Invalid symbol on exchange.", symbol)
        except Exception as e:
            LOG.error("Error scanning symbol %s: %s", symbol, e)
            traceback.print_exc()
        return None

    async def _open_position(self, sig: Signal):
        """
        A robust and hardened hybrid entry method.
        1. Places a market order with a globally unique CID.
        2. Confirms the position exists on the exchange.
        3. Persists to DB to get a position ID (pid).
        4. Places protective orders (SL and either Partial or Final TP) using stable CIDs.
        5. If any step fails, it performs an emergency market-close.
        """
        # --- Steps 1, 2, and 3 (Pre-checks, Entry Order, Confirmation) are correct and remain unchanged ---
        if any(row["symbol"] == sig.symbol for row in self.open_positions.values()):
            return

        try:
            positions = await self.exchange.fetch_positions(symbols=[sig.symbol])
            if positions and positions[0] and float(positions[0].get('info', {}).get('size', 0)) > 0:
                LOG.warning("Skipping entry for %s, pre-flight check found existing exchange position.", sig.symbol)
                return
        except Exception as e:
            LOG.error("Could not perform pre-flight position check for %s: %s", sig.symbol, e)
            return

        bal = await self._fetch_platform_balance()
        free_usdt = bal["free"]["USDT"]
        risk_amt = await self._risk_amount(free_usdt)

        try:
            # Fetch the most current price before calculating size
            ticker = await self.exchange.fetch_ticker(sig.symbol)
            current_price = float(ticker['last'])
            LOG.info(f"Signal price for {sig.symbol} was {sig.entry:.4f}, but current price is {current_price:.4f}.")
        except Exception as e:
            LOG.error(f"Could not fetch live ticker for {sig.symbol} before entry. Aborting. Error: {e}")
            return

        stop_price_signal = current_price + self.cfg["SL_ATR_MULT"] * sig.atr
        stop_dist = abs(current_price - stop_price_signal)
        
        if stop_dist == 0:
            LOG.warning("VETO: Stop distance is zero for %s. Skipping.", sig.symbol)
            return
        intended_size = risk_amt / stop_dist

        try:
            await self._ensure_leverage(sig.symbol)
        except Exception:
            return

        entry_cid = create_unique_cid(f"ENTRY_{sig.symbol}")
        try:
            await self.exchange.create_market_order(
                sig.symbol, "sell", intended_size,
                params={"clientOrderId": entry_cid, "category": "linear"}
            )
            LOG.info("Market order sent for %s. Entry CID: %s", sig.symbol, entry_cid)
        except Exception as e:
            LOG.error("Initial market order placement for %s failed immediately: %s", sig.symbol, e)
            return

        actual_size, actual_entry_price, live_position = 0.0, 0.0, None
        CONFIRMATION_ATTEMPTS = 20
        for i in range(CONFIRMATION_ATTEMPTS):
            await asyncio.sleep(0.5)
            try:
                positions = await self.exchange.fetch_positions(symbols=[sig.symbol])
                pos = next((p for p in positions if p.get('info', {}).get('symbol') == sig.symbol), None)
                if pos and float(pos.get('info', {}).get('size', 0)) > 0:
                    live_position = pos
                    actual_size = float(live_position['info']['size'])
                    actual_entry_price = float(live_position['info']['avgPrice'])
                    LOG.info(f"ENTRY CONFIRMED for %s. Exchange reports size: {actual_size} @ {actual_entry_price}", sig.symbol)
                    break
            except Exception as e:
                LOG.warning("Confirmation loop check failed for %s (attempt %d/%d): %s", sig.symbol, i + 1, CONFIRMATION_ATTEMPTS, e)
        
        if not live_position:
            LOG.error("ENTRY FAILED for %s: Position did not appear on exchange after %d attempts.", sig.symbol, CONFIRMATION_ATTEMPTS)
            return
        
        slippage_usd = (sig.entry - actual_entry_price) * actual_size
        risk_usd = await self._risk_amount(free_usdt) # Capture the intended risk amount
        LOG.info("Slippage for %s entry: $%.4f", sig.symbol, slippage_usd)

        config_snapshot = {
            "SL_ATR_MULT": self.cfg.get("SL_ATR_MULT"),
            "FINAL_TP_ATR_MULT": self.cfg.get("FINAL_TP_ATR_MULT"),
            "PARTIAL_TP_ATR_MULT": self.cfg.get("PARTIAL_TP_ATR_MULT"),
            "PARTIAL_TP_PCT": self.cfg.get("PARTIAL_TP_PCT")
        }


        # --- Step 4: Persist to DB and place protective orders (CRITICAL BLOCK) ---
        try:
            # --- MODIFICATION: Prioritize hourly exit deadline calculation ---
            exit_deadline = None
            if self.cfg.get("TIME_EXIT_HOURS_ENABLED", False):
                exit_deadline = datetime.now(timezone.utc) + timedelta(hours=self.cfg.get("TIME_EXIT_HOURS", 4))
                LOG.info(f"Setting hourly exit for {sig.symbol} at {exit_deadline.isoformat()}")
            elif self.cfg.get("TIME_EXIT_ENABLED", False):
                # Fallback to the daily exit if hourly is disabled
                exit_deadline = datetime.now(timezone.utc) + timedelta(days=self.cfg.get("TIME_EXIT_DAYS", 10))
                LOG.info(f"Setting daily exit for {sig.symbol} at {exit_deadline.isoformat()}")

            pid = await self.db.insert_position({
                "symbol": sig.symbol, "side": "short", "size": actual_size,
                "entry_price": actual_entry_price, "atr": sig.atr,
                "status": "PENDING", "opened_at": datetime.now(timezone.utc),
                "exit_deadline": exit_deadline, # Store the calculated deadline
                # --- ALL NEW DATA ---
                "risk_usd": risk_usd,
                "slippage_usd": slippage_usd,
                "market_regime_at_entry": sig.market_regime,
                "rsi_at_entry": sig.rsi,
                "adx_at_entry": sig.adx,
                "atr_pct_at_entry": sig.atr_pct,
                "price_boom_pct_at_entry": sig.price_boom_pct,
                "price_slowdown_pct_at_entry": sig.price_slowdown_pct,
                "is_ema_crossed_down_at_entry": sig.is_ema_crossed_down,
                "vwap_consolidated_at_entry": sig.vwap_consolidated,
                "vwap_dev_pct_at_entry": sig.vwap_dev_pct,
                "eth_macd_at_entry": eth_macd.get('macd') if eth_macd else None,
                "eth_macdsignal_at_entry": eth_macd.get('signal') if eth_macd else None,
                "eth_macdhist_at_entry": eth_macd.get('hist') if eth_macd else None,
                "ret_30d_at_entry": sig.ret_30d,
                "ema_fast_at_entry": sig.ema_fast,
                "ema_slow_at_entry": sig.ema_slow,
                "listing_age_days_at_entry": sig.listing_age_days,
                "session_tag_at_entry": sig.session_tag,
                "day_of_week_at_entry": sig.day_of_week,
                "hour_of_day_at_entry": sig.hour_of_day,
                "config_snapshot": json.dumps(config_snapshot),
                "win_probability_at_entry": sig.win_probability
            })

            stop_price_actual = actual_entry_price + self.cfg["SL_ATR_MULT"] * sig.atr
            sl_cid = create_stable_cid(pid, "SL")
            
            # FIX: Add closeOnTrigger to the SL params
            sl_params = {
                "triggerPrice": stop_price_actual, "clientOrderId": sl_cid,
                'reduceOnly': True, 'closeOnTrigger': True, 'triggerDirection': 1, 'category': 'linear'
            }
            await self.exchange.create_order(sig.symbol, 'market', "buy", actual_size, None, params=sl_params)

            tp1_cid = None
            tp_final_cid = None

            if self.cfg.get("PARTIAL_TP_ENABLED", False):
                tp1_cid = create_stable_cid(pid, "TP1")
                tp_price = actual_entry_price - self.cfg["PARTIAL_TP_ATR_MULT"] * sig.atr
                qty = actual_size * self.cfg["PARTIAL_TP_PCT"]
                # FIX: Add closeOnTrigger to the TP params
                tp_params = {
                    "triggerPrice": tp_price, "clientOrderId": tp1_cid,
                    'reduceOnly': True, 'closeOnTrigger': True, 'triggerDirection': 2, 'category': 'linear'
                }
                await self.exchange.create_order(sig.symbol, 'market', "buy", qty, None, params=tp_params)
            elif self.cfg.get("FINAL_TP_ENABLED", False):
                tp_final_cid = create_stable_cid(pid, "TP_FINAL")
                tp_price = actual_entry_price - self.cfg["FINAL_TP_ATR_MULT"] * sig.atr
                # FIX: Add closeOnTrigger to the TP params
                tp_params = {
                    "triggerPrice": tp_price, "clientOrderId": tp_final_cid,
                    'reduceOnly': True, 'closeOnTrigger': True, 'triggerDirection': 2, 'category': 'linear'
                }
                await self.exchange.create_order(sig.symbol, 'market', "buy", actual_size, None, params=tp_params)
            
            await self.db.update_position(
                pid, status="OPEN", stop_price=stop_price_actual,
                sl_cid=sl_cid, tp1_cid=tp1_cid, tp_final_cid=tp_final_cid
            )
            row = await self.db.pool.fetchrow("SELECT * FROM positions WHERE id=$1", pid)
            self.open_positions[pid] = dict(row)

            days = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]
            win_prob_pct = sig.win_probability * 100
            await self.tg.send(
                f"üöÄ **({days[sig.day_of_week]})** Opened {sig.symbol} short {actual_size:.3f} @ {actual_entry_price:.4f}\n"
                f"üìà **Win Probability:** {win_prob_pct:.1f}%"
            )


        except Exception as e:
            msg = f"üö® CRITICAL: Failed to persist or protect position for {sig.symbol} due to: {e}. Triggering emergency close."
            LOG.critical(msg)
            await self.tg.send(msg)
            try:
                await self.exchange.create_market_order(
                    sig.symbol, 'buy', actual_size, params={'reduceOnly': True, 'category': 'linear'}
                )
                LOG.warning("Emergency close for %s was successful.", sig.symbol)
            except Exception as close_e:
                msg = f"üö® !!! FAILED TO EMERGENCY CLOSE {sig.symbol}. Manual intervention REQUIRED. Close error: {close_e}"
                LOG.critical(msg)
                await self.tg.send(msg)

    async def _manage_positions_loop(self):
        while True:
            if not self.open_positions:
                await asyncio.sleep(2)
                continue
            for pid, pos in list(self.open_positions.items()):
                try:
                    await self._update_single_position(pid, pos)
                except Exception as e:
                    LOG.error("manage err %s %s", pos["symbol"], e)
            await asyncio.sleep(5)

    async def _update_single_position(self, pid: int, pos: Dict[str, Any]):
        symbol = pos["symbol"]

        # --- FIX: BOT-SIDE SAFETY NET ---
        # First, check the ultimate source of truth: the position size.
        try:
            positions = await self.exchange.fetch_positions(symbols=[symbol])
            position_size = 0.0
            if positions and positions[0]:
                position_size = float(positions[0].get('info', {}).get('size', 0))
            
            if position_size == 0:
                LOG.info("Position size for %s is 0. Trade is closed. Inferring exit reason and finalizing...", symbol)
                
                inferred_reason = "UNKNOWN"
                open_orders = await self._all_open_orders(symbol)
                open_cids = {o.get("clientOrderId") for o in open_orders}
                
                # --- FIX: More robust inference logic ---
                # First, check if any of the possible Take Profit orders are missing.
                if (pos.get("tp_final_cid") and pos["tp_final_cid"] not in open_cids) or \
                (pos.get("tp2_cid") and pos["tp2_cid"] not in open_cids) or \
                (pos.get("tp1_cid") and pos["tp1_cid"] not in open_cids):
                    inferred_reason = "TP"
                # If not a TP, check if any of the possible Stop Loss orders are missing.
                elif (pos.get("sl_trail_cid") and pos["sl_trail_cid"] not in open_cids) or \
                    (pos.get("sl_cid") and pos["sl_cid"] not in open_cids):
                    inferred_reason = "SL"
                
                LOG.info("Inferred exit reason for %s is: %s", symbol, inferred_reason)
                
                await self._finalize_position(pid, pos, inferred_exit_reason=inferred_reason)
                return
        except Exception as e:
            LOG.error("Could not fetch position size for %s during update: %s", symbol, e)
            return
        # --- END OF SAFETY NET ---

        orders = await self._all_open_orders(symbol)
        open_cids = {o.get("clientOrderId") for o in orders}

        if self.cfg.get("TIME_EXIT_ENABLED", cfg.TIME_EXIT_ENABLED):
            ddl = pos.get("exit_deadline")
            if ddl and datetime.now(timezone.utc) >= ddl:
                LOG.info("Time-exit firing on %s (pid %d)", symbol, pid)
                await self._force_close_position(pid, pos, tag="TIME_EXIT")
                return

        if self.cfg.get("PARTIAL_TP_ENABLED", False) and not pos["trailing_active"] and pos.get("tp1_cid") not in open_cids:
            fill_price = None
            try:
                o = await self._fetch_by_cid(pos["tp1_cid"], symbol)
                if o and o.get('status') == 'closed':
                    fill_price = o.get('average') or o.get('price')
            except Exception as e:
                LOG.warning("Failed to fetch TP1 fill price for %s: %s", pos["tp1_cid"], e)

            await self.db.add_fill(
                pid, "TP1", fill_price, float(pos["size"]) * self.cfg["PARTIAL_TP_PCT"], datetime.now(timezone.utc)
            )
            await self.db.update_position(pid, trailing_active=True)
            pos["trailing_active"] = True
            await self._activate_trailing(pid, pos)
            await self.tg.send(f"üìà TP1 hit on {symbol}, trailing activated")

        if pos["trailing_active"]:
            await self._trail_stop(pid, pos)

        active_stop_cid = pos.get("sl_trail_cid") if pos["trailing_active"] else pos.get("sl_cid")
        is_closed = active_stop_cid not in open_cids
        
        if not is_closed and pos["trailing_active"] and self.cfg.get("FINAL_TP_ENABLED", False):
            if pos.get("tp2_cid") not in open_cids:
                is_closed = True

        if is_closed:
            await self._finalize_position(pid, pos)

    async def _activate_trailing(self, pid: int, pos: Dict[str, Any]):
        symbol = pos["symbol"]
        try:
            if pos.get("sl_cid"):
                await self._cancel_by_cid(pos["sl_cid"], symbol)
        except ccxt.OrderNotFound:
            pass
        except Exception as e:
            LOG.warning(f"Could not cancel original SL for {pid} ({pos.get('sl_cid')}): {e}")

        # This places the first trailing stop order
        await self._trail_stop(pid, pos, first=True)

        # FIX: Correctly create, use, and persist the TP2 client order ID
        if self.cfg.get("FINAL_TP_ENABLED", False):
            try:
                final_tp_price = float(pos["entry_price"]) - self.cfg["FINAL_TP_ATR_MULT"] * float(pos["atr"])
                qty_left = float(pos["size"]) * (1 - self.cfg["PARTIAL_TP_PCT"])
                
                # 1. Create a stable, deterministic CID
                tp2_cid = create_stable_cid(pid, "TP2")
                
                await self.exchange.create_order(
                    symbol, "market", "buy", qty_left, None,
                    params={
                        "triggerPrice": final_tp_price, 
                        "clientOrderId": tp2_cid, 
                        'reduceOnly': True, 
                        'closeOnTrigger': True, 
                        'triggerDirection': 2, 
                        'category': 'linear'
                    }
                )
                
                # 2. Persist the new CID to the database
                await self.db.update_position(pid, tp2_cid=tp2_cid)
                
                # 3. Update the in-memory state to match
                pos["tp2_cid"] = tp2_cid
                LOG.info("Final Take Profit (TP2) placed for %s with CID %s", symbol, tp2_cid)

            except Exception as e:
                LOG.error(f"Failed to place final TP2 order for {pid}: {e}")

    async def _trail_stop(self, pid: int, pos: Dict[str, Any], first: bool = False):
        symbol = pos["symbol"]
        price = float((await self.exchange.fetch_ticker(symbol))["last"])
        atr = float(pos["atr"])
        stop_price = float(pos["stop_price"])

        new_stop = price - self.cfg.get("TRAIL_DISTANCE_ATR_MULT", 1.0) * atr
        is_favorable_move = stop_price == 0 or new_stop < stop_price
        min_move_required = price * self.cfg.get("TRAIL_MIN_MOVE_PCT", 0.001)
        is_significant_move = abs(stop_price - new_stop) > min_move_required

        if first or (is_favorable_move and is_significant_move):
            qty_left = float(pos["size"]) * (1 - self.cfg.get("PARTIAL_TP_PCT", 0.7))
            sl_trail_cid = create_stable_cid(pid, "SL_TRAIL")

            try:
                if not first and pos.get("sl_trail_cid"):
                    await self._cancel_by_cid(pos["sl_trail_cid"], symbol)
            except ccxt.OrderNotFound:
                pass
            except Exception as e:
                LOG.warning("Trail cancel failed for %s: %s. Will retry.", symbol, e)
                return
            
            # FIX: Add closeOnTrigger to the trailing SL params
            sl_params = {
                "triggerPrice": new_stop, "clientOrderId": sl_trail_cid,
                'reduceOnly': True, 'closeOnTrigger': True, 'triggerDirection': 1, 'category': 'linear'
            }
            await self.exchange.create_order(symbol, 'market', "buy", qty_left, None, params=sl_params)
            
            await self.db.update_position(pid, stop_price=new_stop, sl_trail_cid=sl_trail_cid)
            pos["stop_price"] = new_stop
            pos["sl_trail_cid"] = sl_trail_cid
            LOG.info("Trail updated %s to %.4f", symbol, new_stop)
            
    async def _finalize_position(self, pid: int, pos: Dict[str, Any], inferred_exit_reason: str = None):
            symbol = pos["symbol"]
            opened_at = pos["opened_at"]
            entry_price_float = float(pos["entry_price"])
            size = float(pos["size"])
            closed_at = datetime.now(timezone.utc)
            exit_price, exit_qty, closing_order_type = None, 0.0, "UNKNOWN"

            # --- 1. Determine the type of exit ---
            closing_order_cid = None
            if inferred_exit_reason:
                closing_order_type = inferred_exit_reason
                if inferred_exit_reason == "TP":
                    closing_order_cid = pos.get("tp_final_cid") or pos.get("tp2_cid") or pos.get("tp1_cid")
                elif inferred_exit_reason == "SL":
                    closing_order_cid = pos.get("sl_trail_cid") or pos.get("sl_cid")

            # --- 2. Get the Accurate Exit Price ---
            # First, try to fetch the order by its ID. This is the fastest method.
            if closing_order_cid:
                try:
                    order = await self._fetch_by_cid(closing_order_cid, symbol)
                    if order and (order.get('average') or order.get('price')):
                        exit_price = float(order.get('average') or order.get('price'))
                        exit_qty = float(order.get('filled', 0))
                        LOG.info(f"Found closing order {closing_order_cid}. Exit Price: {exit_price}")
                except Exception as e:
                    LOG.warning("Could not fetch closing order by CID %s: %s. Using robust fallback.", closing_order_cid, e)

            # --- THIS IS THE CRITICAL FIX: A ROBUST FALLBACK ---
            # If the primary method fails, fetch our actual trade history. This is the ground truth.
            if not exit_price:
                LOG.warning("Primary exit price fetch failed for {symbol}. Using fetch_my_trades() as the definitive fallback.")
                try:
                    # Wait a moment for the trade to be registered by the exchange API
                    await asyncio.sleep(1.5)
                    my_trades = await self.exchange.fetch_my_trades(symbol, limit=5)
                    # The closing trade is the most recent 'buy' order (to close our short)
                    closing_trade = next((t for t in reversed(my_trades) if t.get('side') == 'buy'), None)
                    
                    if closing_trade:
                        exit_price = float(closing_trade['price'])
                        exit_qty = float(closing_trade['amount'])
                        closing_order_type = inferred_exit_reason or "FALLBACK_FILL"
                        LOG.info(f"Confirmed closing fill for {symbol} via fallback. Exit Price: {exit_price}")
                    else:
                        # This should almost never happen
                        LOG.error(f"CRITICAL: Could not find a closing fill for {symbol} via any method. PnL will be inaccurate.")
                        exit_price = entry_price_float # Set PnL to zero to flag the error
                        exit_qty = size
                except Exception as e:
                    LOG.error(f"CRITICAL: Fallback to fetch_my_trades for {symbol} also failed: {e}. PnL will be inaccurate.")
                    exit_price = entry_price_float # Set PnL to zero to flag the error
                    exit_qty = size
            # --- END OF FIX ---

            # --- 3. Calculate All Metrics with the Accurate Price ---
            total_pnl = (entry_price_float - exit_price) * size
            holding_minutes = (closed_at - opened_at).total_seconds() / 60 if opened_at else 0.0
            pnl_pct = ((entry_price_float / exit_price) - 1) * 100 if exit_price > 0 else 0.0
            
            mae_usd, mfe_usd, mae_over_atr, mfe_over_atr = 0.0, 0.0, 0.0, 0.0
            realized_vol_during_trade, btc_beta_during_trade = 0.0, 0.0

            try:
                if closing_order_type == "SL":
                    mae_usd = (exit_price - entry_price_float) * size
                elif closing_order_type in ["TP", "TP1", "TP2", "TP_FINAL", "FALLBACK_FILL"]:
                    mfe_usd = (entry_price_float - exit_price) * size

                since_ts = int(opened_at.timestamp() * 1000)
                ohlcv = await self.exchange.fetch_ohlcv(symbol, '1m', since=since_ts)
                
                if ohlcv:
                    trade_df = pd.DataFrame(ohlcv, columns=['ts', 'o', 'h', 'l', 'c', 'v'])
                    trade_df['ts'] = pd.to_datetime(trade_df['ts'], unit='ms', utc=True)
                    trade_df = trade_df[trade_df['ts'] <= closed_at]

                    if not trade_df.empty:
                        if mae_usd == 0.0:
                            mae_usd = (trade_df['h'].max() - entry_price_float) * size
                        if mfe_usd == 0.0:
                            mfe_usd = (entry_price_float - trade_df['l'].min()) * size

                        asset_returns = trade_df['c'].pct_change().dropna()
                        if not asset_returns.empty:
                            realized_vol_during_trade = asset_returns.std() * np.sqrt(365 * 24 * 60)

                        benchmark_symbol = self.cfg.get("REGIME_BENCHMARK_SYMBOL", "BTCUSDT")
                        if symbol != benchmark_symbol:
                            btc_ohlcv = await self.exchange.fetch_ohlcv(benchmark_symbol, '1m', since=since_ts)
                            if btc_ohlcv:
                                btc_df = pd.DataFrame(btc_ohlcv, columns=['ts', 'o', 'h', 'l', 'c', 'v'])
                                btc_df['ts'] = pd.to_datetime(btc_df['ts'], unit='ms', utc=True)
                                btc_df = btc_df[btc_df['ts'] <= closed_at]
                                if not btc_df.empty:
                                    asset_returns_named = asset_returns.rename('asset')
                                    btc_returns_named = btc_df['c'].pct_change().rename('btc')
                                    combined_df = pd.concat([asset_returns_named, btc_returns_named], axis=1).dropna()
                                    if len(combined_df) > 5:
                                        covariance = combined_df['asset'].cov(combined_df['btc'])
                                        variance = combined_df['btc'].var()
                                        if variance > 0:
                                            btc_beta_during_trade = covariance / variance

                atr_at_entry = float(pos["atr"])
                if atr_at_entry > 0:
                    mae_usd = max(0, mae_usd)
                    mfe_usd = max(0, mfe_usd)
                    mae_over_atr = (mae_usd / size) / atr_at_entry
                    mfe_over_atr = (mfe_usd / size) / atr_at_entry
            except Exception as e:
                LOG.warning("Could not calculate advanced post-trade metrics for %s: %s", symbol, e)

            # --- 4. Finalize and Clean Up ---
            try:
                await self.exchange.cancel_all_orders(symbol, params={'category': 'linear'})
            except Exception as e:
                LOG.warning(f"Final cleanup for position {pid} failed: {e}.")

            await self.db.add_fill(pid, closing_order_type, exit_price, exit_qty, closed_at)

            # Note: The previous logic to re-calculate PnL from all fills is now redundant
            # because we are getting the accurate exit price. We will use the direct calculation.

            await self.db.update_position(
                pid, status="CLOSED", closed_at=closed_at, pnl=total_pnl,
                exit_reason=closing_order_type, holding_minutes=holding_minutes,
                pnl_pct=pnl_pct, mae_usd=mae_usd, mfe_usd=mfe_usd,
                mae_over_atr=mae_over_atr, mfe_over_atr=mfe_over_atr,
                realized_vol_during_trade=realized_vol_during_trade,
                btc_beta_during_trade=btc_beta_during_trade
            )
            
            await self.risk.on_trade_close(total_pnl, self.tg)
            self.open_positions.pop(pid, None)
            self.last_exit[symbol] = closed_at
            await self.tg.send(f"‚úÖ {symbol} position closed. Total PnL ‚âà {total_pnl:.2f} USDT")

    async def _force_open_position(self, symbol: str):
        """
        Manually triggers a trade for a given symbol for testing purposes.
        It runs a full scan to gather real data but bypasses the signal logic.
        """
        await self.tg.send(f"Received force open command for {symbol}. Attempting to generate signal data...")
        LOG.info("Manual trade requested via Telegram for symbol: %s", symbol)

        # --- Safety Check 1: Is a position already open? ---
        if any(p['symbol'] == symbol for p in self.open_positions.values()):
            msg = f"‚ö†Ô∏è Cannot force open {symbol}: A position is already open for this symbol."
            LOG.warning(msg)
            await self.tg.send(msg)
            return

        # --- Gather real-time data ---
        try:
            # We need the current market regime to run the scan
            current_market_regime = await self.regime_detector.get_current_regime()
            
            # Run the scanner to get a fully populated Signal object with real data
            signal = await self._scan_symbol_for_signal(symbol, current_market_regime)

            if signal is None:
                # This can happen if the exchange fails to return data for the symbol
                msg = f"‚ùå Failed to force open {symbol}: Could not generate signal data. The symbol may be invalid or exchange data is unavailable."
                LOG.error(msg)
                await self.tg.send(msg)
                return
                
            # --- Safety Check 2: Run the filters ---
            # Even though we bypass the signal, we should still respect the main filters (max positions, etc.)
            equity = await self.db.latest_equity() or 0.0
            open_positions_count = len(self.open_positions)
            ok, vetoes = filters.evaluate(
                signal, listing_age_days=signal.listing_age_days,
                open_positions=open_positions_count, equity=equity
            )

            if not ok:
                msg = f"‚ö†Ô∏è VETOED force open for {symbol}: {' | '.join(vetoes)}"
                LOG.warning(msg)
                await self.tg.send(msg)
                return

            # --- Execute the trade ---
            LOG.info("Bypassing signal conditions and proceeding to open position for %s.", symbol)
            await self.tg.send(f"‚úÖ Signal data generated for {symbol}. Proceeding with forced entry.")
            await self._open_position(signal)

        except Exception as e:
            msg = f"‚ùå An unexpected error occurred during force open for {symbol}: {e}"
            LOG.error(msg, exc_info=True)
            await self.tg.send(msg)

    async def _force_close_position(self, pid: int, pos: Dict[str, Any], tag: str):
            symbol = pos["symbol"]
            size = float(pos["size"])
            entry_price = float(pos["entry_price"])
            
            try:
                # 1. Cancel any other open orders (like SL/TP) for this position
                await self.exchange.cancel_all_orders(symbol)
                
                # 2. Send the market order to close the position
                await self.exchange.create_market_order(symbol, "buy", size, params={'reduceOnly': True})
                LOG.info(f"Force-closing {symbol} (pid {pid}) with a market order due to: {tag}")

            except Exception as e:
                LOG.warning("Force-close order issue on %s: %s", symbol, e)

            # --- THIS IS THE CRITICAL FIX ---
            # 3. Wait a moment for the trade to register on the exchange
            await asyncio.sleep(2) # 2-second buffer

            # 4. Fetch the actual fill price from our trade history
            exit_price = None
            try:
                # Fetch our most recent trades for this symbol
                my_trades = await self.exchange.fetch_my_trades(symbol, limit=5)
                # The closing trade is the most recent 'buy' order
                closing_trade = next((t for t in reversed(my_trades) if t.get('side') == 'buy'), None)
                
                if closing_trade:
                    exit_price = float(closing_trade['price'])
                    LOG.info(f"Confirmed force-close fill price for {symbol} at {exit_price}")
                else:
                    LOG.warning(f"Could not confirm fill price for force-close of {symbol} via fetch_my_trades. Falling back to ticker.")
            except Exception as e:
                LOG.error(f"Error fetching fill price for force-close of {symbol}: {e}")

            # 5. Last resort fallback (should rarely be used now)
            if not exit_price:
                exit_price = float((await self.exchange.fetch_ticker(symbol))["last"])

            # 6. Calculate PnL with the accurate price
            pnl = (entry_price - exit_price) * size
            closed_at = datetime.now(timezone.utc)
            holding_minutes = (closed_at - pos["opened_at"]).total_seconds() / 60 if pos.get("opened_at") else 0.0
            
            # 7. Update the database with all the correct information
            await self.db.update_position(
                pid, 
                status="CLOSED", 
                closed_at=closed_at, 
                pnl=pnl,
                exit_reason=tag, # Use the tag (e.g., "TIME_EXIT") as the reason
                holding_minutes=holding_minutes
            )
            # --- END OF FIX ---
            
            await self.db.add_fill(pid, tag, exit_price, size, closed_at)
            await self.risk.on_trade_close(pnl, self.tg)
            self.last_exit[symbol] = closed_at
            self.open_positions.pop(pid, None)
            
            await self.tg.send(f"‚è∞ {symbol} closed by {tag}. PnL ‚âà {pnl:.2f} USDT")

    async def _main_signal_loop(self):
        LOG.info("Starting main signal scan loop.")
        while True:
            try:
                if self.paused or not self.risk.can_trade():
                    await asyncio.sleep(5)
                    continue

                current_market_regime = await self.regime_detector.get_current_regime()
                LOG.info("Starting new scan cycle for %d symbols with market regime: %s", len(self.symbols), current_market_regime)

                # --- NEW: Calculate the ETH MACD Barometer ---
                eth_macd_data = None
                try:
                    # Fetch 4-hour data for ETHUSDT
                    eth_ohlcv = await self.exchange.fetch_ohlcv('ETHUSDT', '4h', limit=100)
                    if eth_ohlcv:
                        df_eth = pd.DataFrame(eth_ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
                        # Calculate MACD using your indicators module
                        macd_df = ta.macd(df_eth['close'])
                        # Get the very last (most recent) row of MACD data
                        latest_macd = macd_df.iloc[-1]
                        eth_macd_data = {
                            "macd": latest_macd['macd'],
                            "signal": latest_macd['signal'],
                            "hist": latest_macd['hist']
                        }
                        LOG.info(f"ETH Barometer MACD(4h): {latest_macd['macd']:.2f}, Hist: {latest_macd['hist']:.2f}")
                except Exception as e:
                    LOG.warning(f"Could not calculate ETH MACD barometer: {e}")
                # --- END OF NEW SECTION ---

                equity = await self.db.latest_equity() or 0.0
                open_positions_count = len(self.open_positions)
                open_symbols = {p['symbol'] for p in self.open_positions.values()}

                for sym in self.symbols:
                    if self.paused or not self.risk.can_trade():
                        break
                    
                    if sym in open_symbols:
                        continue

                    cd_h = self.cfg.get("SYMBOL_COOLDOWN_HOURS", cfg.SYMBOL_COOLDOWN_HOURS)
                    last_x = self.last_exit.get(sym)
                    if last_x and datetime.now(timezone.utc) - last_x < timedelta(hours=cd_h):
                        continue

                    async with self.symbol_locks[sym]:
                        # FIX 1.4: This is the single, correct pre-flight check location
                        try:
                            positions = await self.exchange.fetch_positions(symbols=[sym])
                            if positions and positions[0] and float(positions[0].get('info', {}).get('size', 0)) > 0:
                                LOG.info("Skipping scan for %s, pre-flight check found position already exists on exchange.", sym)
                                if sym not in open_symbols:
                                    LOG.warning("ORPHAN POSITION DETECTED for %s during pre-flight check! Reconciliation needed.", sym)
                                continue
                        except Exception as e:
                            LOG.error("Could not perform pre-flight position check for %s: %s", sym, e)
                            continue

                        signal = await self._scan_symbol_for_signal(sym, current_market_regime, eth_macd_data)
                        if signal:
                            age = (datetime.utcnow().date() - self._listing_dates_cache[sym]).days if sym in self._listing_dates_cache else None
                            ok, vetoes = filters.evaluate(
                                signal, listing_age_days=age,
                                open_positions=open_positions_count, equity=equity
                            )
                            if ok:
                                await self._open_position(signal)
                                open_positions_count += 1
                                open_symbols.add(sym)
                            else:
                                LOG.info("Signal for %s vetoed: %s", sym, " | ".join(vetoes))
                    
                    await asyncio.sleep(0.5)

            except Exception as e:
                LOG.error("Critical error in main signal loop: %s", e)
                traceback.print_exc()

            LOG.info("Scan cycle complete. Waiting for next interval.")
            await asyncio.sleep(self.cfg.get("SCAN_INTERVAL_SEC", 60))

    async def _equity_loop(self):
        while True:
            try:
                bal = await self._fetch_platform_balance()
                current_equity = bal["total"]["USDT"]
                await self.db.snapshot_equity(current_equity, datetime.now(timezone.utc))

                if current_equity > self.peak_equity:
                    self.peak_equity = current_equity

                if self.cfg.get("DD_PAUSE_ENABLED", True) and not self.risk.kill_switch:
                    if self.peak_equity and current_equity < self.peak_equity:
                        drawdown_pct = (self.peak_equity - current_equity) / self.peak_equity * 100
                        max_dd_pct = self.cfg.get("DD_MAX_PCT", 10.0)
                        if drawdown_pct >= max_dd_pct:
                            self.risk.kill_switch = True
                            msg = f"‚ùå KILL-SWITCH: Equity drawdown of {drawdown_pct:.2f}% exceeded {max_dd_pct}%."
                            LOG.warning(msg)
                            await self.tg.send(msg)
            except Exception as e:
                LOG.error("Error in equity loop: %s", e)
            await asyncio.sleep(3600)

    async def _telegram_loop(self):
        while True:
            async for cmd in self.tg.poll_cmds():
                await self._handle_cmd(cmd)
            await asyncio.sleep(1)

    async def _handle_cmd(self, cmd: str):
        parts = cmd.split()
        root = parts[0].lower()
        if root == "/pause":
            self.paused = True
            await self.tg.send("‚è∏ Paused")

        elif root == "/report" and len(parts) == 2:
            period = parts[1].lower() # e.g., 'daily', 'weekly', '6h'
            if period in ['6h', 'daily', 'weekly', 'monthly']:
                await self.tg.send(f"Generating on-demand '{period}' report...")
                summary_text = await self._generate_summary_report(period)
                await self.tg.send(summary_text)
            else:
                await self.tg.send(f"Unknown period '{period}'. Use: 6h, daily, weekly, monthly.")        

        elif root == "/resume":
            if self.risk.can_trade():
                self.paused = False
                await self.tg.send("‚ñ∂Ô∏è Resumed")
            else:
                await self.tg.send("‚ö†Ô∏è Kill switch active")
        elif root == "/set" and len(parts) == 3:
            key, val = parts[1], parts[2]
            try:
                cast = json.loads(val)
            except json.JSONDecodeError:
                cast = val
            self.cfg[key] = cast
            await self.tg.send(f"‚úÖ {key} set to {cast}")

        elif root == "/open" and len(parts) == 2:
            symbol = parts[1].upper() # e.g., BTCUSDT
            # We run this as a background task so it doesn't block the Telegram loop
            # while it fetches data and opens the trade.
            asyncio.create_task(self._force_open_position(symbol))
            return # Acknowledge the command immediately

        elif root == "/status":
            await self.tg.send(json.dumps({
                "paused": self.paused,
                "open": len(self.open_positions),
                "loss_streak": self.risk.loss_streak
            }, indent=2))

        elif root == "/analyze":
            await self.tg.send("ü§ñ Roger that. Starting the full analysis process. This requires the `bybit.csv` file to be present. The process may take a few minutes. I will send the report file here when complete.")
            
            # Run the master script as a non-blocking background process
            try:
                subprocess.Popen(["/opt/livefader/src/run_weekly_report.sh"])
            except Exception as e:
                await self.tg.send(f"‚ùå Failed to start the analysis script: {e}")
            return

    async def _resume(self):
            """
            On startup, intelligently load state from DB and reconcile with the exchange.
            This version identifies valid positions FIRST, then cleans up only true orphans,
            and includes all necessary reconciliation checks.
            """
            LOG.info("--> Resuming state with intelligent reconciliation...")

            # --- STEP 1: Fetch current state from both Exchange and Database ---
            LOG.info("Step 1: Fetching open positions from EXCHANGE and DATABASE...")
            try:
                exchange_positions = await self.exchange.fetch_positions()
                open_exchange_positions = {
                    p['info']['symbol']: p for p in exchange_positions if float(p['info'].get('size', 0)) > 0
                }
                LOG.info("...Success! Found %d positions on the exchange.", len(open_exchange_positions))
            except Exception as e:
                LOG.error("CRITICAL: Could not fetch exchange positions on startup: %s. Exiting.", e)
                sys.exit(1)

            db_positions_rows = await self.db.fetch_open_positions()
            db_positions = {r["symbol"]: dict(r) for r in db_positions_rows}
            LOG.info("...Success! Found %d 'OPEN' positions in the database.", len(db_positions))

            # --- STEP 2: Reconcile positions and identify all VALID client order IDs ---
            LOG.info("Step 2: Reconciling positions and identifying valid protective orders...")
            valid_cids = set()
            now_utc = datetime.now(timezone.utc)

            # Check for orphan positions on the exchange
            for symbol, pos_data in open_exchange_positions.items():
                if symbol not in db_positions:
                    msg = f"üö® ORPHAN DETECTED: Position for {symbol} exists on exchange but not in DB. Closing immediately."
                    LOG.warning(msg)
                    await self.tg.send(msg)
                    try:
                        side = 'buy' if pos_data['side'] == 'short' else 'sell'
                        size = float(pos_data['info']['size'])
                        await self.exchange.create_market_order(symbol, side, size, params={'reduceOnly': True})
                    except Exception as e:
                        LOG.error("Failed to force-close orphan position %s: %s", symbol, e)

            # Check for DB/exchange mismatches and build the "safe list" of CIDs
            for symbol, pos_row in list(db_positions.items()):
                pid = pos_row["id"]
                
                if symbol not in open_exchange_positions:
                    LOG.warning(f"DB/EXCHANGE MISMATCH: Position {pid} for {symbol} is 'OPEN' in DB but not on exchange. Marking as closed.")
                    await self.db.update_position(pid, status="CLOSED", closed_at=now_utc, pnl=0, exit_reason="RECONCILE_CLOSE")
                    del db_positions[symbol]
                    continue

                # --- NEW: Check for stale positions on restart ---
                opened_at = pos_row.get("opened_at")
                max_holding_duration = None
                if self.cfg.get("TIME_EXIT_HOURS_ENABLED", False):
                    max_holding_duration = timedelta(hours=self.cfg.get("TIME_EXIT_HOURS", 4))
                elif self.cfg.get("TIME_EXIT_ENABLED", False):
                    max_holding_duration = timedelta(days=self.cfg.get("TIME_EXIT_DAYS", 10))

                if opened_at and max_holding_duration and (now_utc - opened_at) > max_holding_duration:
                    msg = f"‚è∞ STALE POSITION DETECTED on restart: {symbol} (pid {pid}) is older than the time limit. Forcing close."
                    LOG.warning(msg)
                    await self.tg.send(msg)
                    # Use create_task to close in the background without blocking startup
                    asyncio.create_task(self._force_close_position(pid, pos_row, tag="STALE_ON_RESTART"))
                    # Remove from the list of positions to be loaded into memory
                    del db_positions[symbol]
                    continue # Skip to the next position

                # --- END OF NEW LOGIC ---

                ex_pos = open_exchange_positions[symbol]
                db_size = float(pos_row['size'])
                ex_size = float(ex_pos['info']['size'])
                if abs(db_size - ex_size) > 1e-9:
                    msg = (f"üö® SIZE MISMATCH on resume for {symbol} (pid {pid}): DB size is {db_size}, Exchange size is {ex_size}. Flagging for manual review.")
                    LOG.critical(msg)
                    await self.tg.send(msg)
                    await self.db.update_position(pid, status="SIZE_MISMATCH")
                    del db_positions[symbol]
                    continue

                LOG.debug("Identified valid CIDs for position %s to preserve.", symbol)
                if pos_row.get("sl_cid"): valid_cids.add(pos_row["sl_cid"])
                if pos_row.get("tp1_cid"): valid_cids.add(pos_row["tp1_cid"])
                if pos_row.get("tp_final_cid"): valid_cids.add(pos_row["tp_final_cid"])
                if pos_row.get("sl_trail_cid"): valid_cids.add(pos_row["sl_trail_cid"])

            LOG.info("...Identified %d CIDs belonging to valid positions.", len(valid_cids))

            # --- STEP 3: Perform the INTELLIGENT Clean Slate Protocol ---
            LOG.info("Step 3: Fetching all open orders to clean up ORPHANS ONLY...")
            try:
                all_open_orders = await self._all_open_orders_for_all_symbols()
                
                cancel_tasks = []
                for order in all_open_orders:
                    cid = order.get("clientOrderId")
                    if cid and cid.startswith("bot_") and cid not in valid_cids:
                        LOG.warning("Found orphaned order %s for symbol %s. Scheduling for cancellation.", cid, order['symbol'])
                        cancel_tasks.append(self.exchange.cancel_order(order['id'], order['symbol'], params={'category': 'linear'}))
                
                if cancel_tasks:
                    LOG.info("Cancelling %d orphaned orders...", len(cancel_tasks))
                    await asyncio.gather(*cancel_tasks, return_exceptions=True)
                    LOG.info("...Orphaned order cleanup complete.")
                else:
                    LOG.info("...No orphaned orders found to cancel.")
                    
            except Exception as e:
                LOG.error("CRITICAL: Failed to perform clean slate protocol on startup: %s", e)
                traceback.print_exc()

            # --- STEP 4: Load final state into memory ---
            LOG.info("Step 4: Loading final reconciled positions into memory...")
            # The db_positions dict now only contains valid, non-stale positions
            for symbol, pos_row in db_positions.items():
                self.open_positions[pos_row["id"]] = pos_row
                LOG.info("...Successfully resumed and verified open position for %s (ID: %d)", symbol, pos_row["id"])

            LOG.info("Step 5: Fetching peak equity from DB...")
            peak = await self.db.pool.fetchval("SELECT MAX(equity) FROM equity_snapshots")
            self.peak_equity = float(peak) if peak is not None else 0.0
            LOG.info("...Initial peak equity loaded: $%.2f", self.peak_equity)

            LOG.info("Step 6: Loading recent exit timestamps for cooldowns...")
            cd_h = int(self.cfg.get("SYMBOL_COOLDOWN_HOURS", cfg.SYMBOL_COOLDOWN_HOURS))
            rows = await self.db.pool.fetch(
                "SELECT symbol, closed_at FROM positions "
                "WHERE status='CLOSED' AND closed_at > (NOW() AT TIME ZONE 'utc') - $1::interval",
                timedelta(hours=cd_h),
            )
            for r in rows:
                self.last_exit[r["symbol"]] = r["closed_at"]
            
            LOG.info("<-- Resume complete.")

    async def run(self):
        await self.db.init()
        
        if self.settings.bybit_testnet:
            LOG.warning("="*60)
            LOG.warning("RUNNING ON TESTNET")
            LOG.warning("Testnet data is unreliable for most altcoins.")
            LOG.warning("Signals may be incorrect. Use for order logic testing only.")
            LOG.warning("="*60)

        LOG.info("Loading exchange market data...")
        try:
            await self.exchange._exchange.load_markets()
            LOG.info("Market data loaded.")
        except Exception as e:
            LOG.error("Could not load exchange market data: %s. Exiting.", e)
            return

        LOG.info("Loading symbol listing dates...")
        self._listing_dates_cache = await self._load_listing_dates()

        await self._resume()
        await self.tg.send("ü§ñ Bot online v6.0")

        try:
            async with asyncio.TaskGroup() as tg:
                tg.create_task(self._main_signal_loop())
                tg.create_task(self._manage_positions_loop())
                tg.create_task(self._telegram_loop())
                tg.create_task(self._equity_loop())
                tg.create_task(self._reporting_loop())
            LOG.info("All tasks finished gracefully.")
        except* (asyncio.CancelledError, KeyboardInterrupt):
            LOG.info("Shutdown signal received. Closing connections...")
        finally:
            await self.exchange.close()
            if self.db.pool:
                await self.db.pool.close()
            await self.tg.close()
            LOG.info("Bot shut down cleanly.")

    async def _generate_summary_report(self, period: str) -> str:
        """
        Queries the database for a given period, calculates KPIs, and returns a formatted summary string.
        
        Args:
            period: A string like '6h', 'daily', 'weekly', or 'monthly'.
        """
        now = datetime.now(timezone.utc)
        period_map = {
            '6h': timedelta(hours=6),
            'daily': timedelta(days=1),
            'weekly': timedelta(weeks=1),
            'monthly': timedelta(days=30) # Approximation for monthly
        }
        
        if period not in period_map:
            return f"Error: Unknown report period '{period}'."

        start_time = now - period_map[period]
        LOG.info("Generating %s summary report for trades closed since %s", period, start_time.isoformat())

        try:
            query = """
                SELECT
                    pnl
                FROM positions
                WHERE status = 'CLOSED' AND closed_at >= $1
            """
            records = await self.db.pool.fetch(query, start_time)

            if not records:
                return f"üìä *{period.capitalize()} Report*\n\nNo trades were closed in the last {period}."

            total_trades = len(records)
            pnl_values = [float(r['pnl']) for r in records if r['pnl'] is not None]
            
            wins = [p for p in pnl_values if p > 0]
            losses = [p for p in pnl_values if p < 0]
            
            win_count = len(wins)
            loss_count = len(losses)
            
            win_rate = (win_count / total_trades) * 100 if total_trades > 0 else 0
            total_pnl = sum(pnl_values)
            
            gross_profit = sum(wins)
            gross_loss = abs(sum(losses))
            
            profit_factor = gross_profit / gross_loss if gross_loss > 0 else float('inf')
            
            avg_win = sum(wins) / win_count if win_count > 0 else 0
            avg_loss = sum(losses) / loss_count if loss_count > 0 else 0
            
            expectancy = (avg_win * (win_rate / 100)) - (abs(avg_loss) * (1 - (win_rate / 100)))

            # Using MarkdownV2 for formatting
            report_lines = [
                f"üìä *{period.capitalize()} Performance Summary*",
                f"```{'-'*25}",
                f" Period: Last {period}",
                f" Total Closed Trades: {total_trades}",
                f" Total PnL: {total_pnl:+.2f} USDT",
                f"",
                f" Win Rate: {win_rate:.2f}% ({win_count} W / {loss_count} L)",
                f" Profit Factor: {profit_factor:.2f}",
                f" Expectancy/Trade: {expectancy:+.2f} USDT",
                f"",
                f" Avg Win:  {avg_win:+.2f} USDT",
                f" Avg Loss: {avg_loss:+.2f} USDT",
                f"```{'-'*25}",
            ]
            
            return "\n".join(report_lines)

        except Exception as e:
            LOG.error("Failed to generate summary report: %s", e)
            return f"Error: Could not generate {period} report. Check logs."

    async def _reporting_loop(self):
        """A background loop that sends scheduled reports to Telegram."""
        LOG.info("Reporting loop started.")
        last_report_sent = {} # Stores the last time a report was sent for a period

        while True:
            await asyncio.sleep(60 * 5) # Check every 5 minutes
            now = datetime.now(timezone.utc)
            
            periods_to_check = {
                '6h': now.hour % 6 == 0,
                'daily': now.hour == 0,
                'weekly': now.weekday() == 0 and now.hour == 0, # Monday morning
            }
            
            for period, should_send in periods_to_check.items():
                # To prevent spam on restart, we check if the last report for this period
                # was already sent within the current time window.
                last_sent_date = last_report_sent.get(period)
                
                # For daily/weekly, we just check the date. For 6h, we check the hour block.
                is_already_sent = False
                if period in ['daily', 'weekly'] and last_sent_date == now.date():
                    is_already_sent = True
                elif period == '6h' and last_sent_date == (now.date(), now.hour // 6):
                    is_already_sent = True

                if should_send and not is_already_sent:
                    LOG.info("Triggering scheduled '%s' report.", period)
                    summary_text = await self._generate_summary_report(period)
                    await self.tg.send(summary_text)
                    
                    # Mark as sent for this time window
                    if period in ['daily', 'weekly']:
                        last_report_sent[period] = now.date()
                    elif period == '6h':
                        last_report_sent[period] = (now.date(), now.hour // 6)

###############################################################################
# 6 ‚ñ∏ ENTRYPOINT ##############################################################
###############################################################################


async def async_main():
    try:
        settings = Settings()
    except ValidationError as e:
        LOG.error("Bad env: %s", e)
        sys.exit(1)

    cfg_dict = load_yaml(CONFIG_PATH)
    trader = LiveTrader(settings, cfg_dict)
    await trader.run()


if __name__ == "__main__":
    asyncio.run(async_main())